{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Change path to point to the root folder \n",
    "sys.path.insert(0, \"<credit-scoring-root-dir>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from src.data_preprocessing import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data_path = \"../data/credit_ds_v3.csv\"\n",
    "df = read_dataset(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null rows before split\n",
    "df = drop_nulls(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X = df.drop('default_flag', axis = 1)\n",
    "y = df['default_flag']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 42, stratify = y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions: process_features, numerical_features_binning\n",
    "# are located in root/src/\n",
    "\n",
    "processed_features = FunctionTransformer(process_features)\n",
    "binned_features = FunctionTransformer(numerical_features_binning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greatest threat to financial sustainability arises when the classifier predicts that a customer will repay their credit and they actually default (FP). Therefore, it is most important to correctly predict the customers who will not repay. For applications that require highly effective detection ability for only one class, it is recommended to consider an alternative metric to accuracy. The credit scoring application is best assessed using the classification metric known as recall. In the case of credit scoring, predicting defaulters is of utmost importance.\n",
    "\n",
    "In addition, the metric AUROC or AUC-ROC score (Area Under the Receiver Operating Characteristics) is one of the most commonly used measures for evaluating predictive performance.\n",
    "\n",
    "In this case, the AUROC discriminates customers with a higher credit risk propensity from the good ones. A limitation of the AUROC is that it does not capture the proper performance of models built for datasets with a much larger quantity of negative examples than positive examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model used will be a Linear Regression model that is performed on the original dataset without any enhancements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model on dataset\n",
    "\n",
    "def baseline(data_path):\n",
    "    df = read_dataset(data_path)\n",
    "    df = df.dropna()\n",
    "    numerical_ix = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    df = df[numerical_ix]\n",
    "    X = df.drop('default_flag', axis=1)\n",
    "    y = df['default_flag']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    results = roc_auc_score(y_test, y_pred)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network():\n",
    "    # define the layers\n",
    "    x_in = Input(shape=(42,))  # This is number of columns after processing\n",
    "    x1 = layers.Dense(64, activation=\"relu\")(x_in)\n",
    "    d = layers.Dropout(0.2)(x1)\n",
    "    x2 = layers.Dense(32, activation=\"relu\")(d)\n",
    "    x_out = layers.Dense(1, activation=\"sigmoid\")(x2)\n",
    "    # define the model\n",
    "    model = Model(inputs=x_in, outputs=x_out)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models with a class weighting\n",
    "models_dict = {\n",
    "    \"weighted_random_forest\": RandomForestClassifier(n_estimators=30, max_depth=9, class_weight='balanced'),\n",
    "    \"weighted_svm\": LinearSVC(C=0.0001, class_weight='balanced'),\n",
    "    \"weighted_xgboostclassifier\": XGBClassifier(n_estimators=30, max_depth=9, scale_pos_weight=10),\n",
    "    \"neural_network\": KerasClassifier(build_fn=neural_network, verbose=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline AUROC score is: 0.738442822384428\n"
     ]
    }
   ],
   "source": [
    "print(f\"Baseline AUROC score is: {baseline(data_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: weighted_random_forest\n",
      "Confusion matrix: \n",
      "[[2020  809]\n",
      " [  79  190]]\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.71      0.82      2829\n",
      "           1       0.19      0.71      0.30       269\n",
      "\n",
      "    accuracy                           0.71      3098\n",
      "   macro avg       0.58      0.71      0.56      3098\n",
      "weighted avg       0.90      0.71      0.77      3098\n",
      "\n",
      "Area under ROC: \n",
      "0.7101764649455126\n",
      "-----------------------------------------------------------\n",
      "Model: weighted_svm\n",
      "Confusion matrix: \n",
      "[[1811 1018]\n",
      " [  60  209]]\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.64      0.77      2829\n",
      "           1       0.17      0.78      0.28       269\n",
      "\n",
      "    accuracy                           0.65      3098\n",
      "   macro avg       0.57      0.71      0.53      3098\n",
      "weighted avg       0.90      0.65      0.73      3098\n",
      "\n",
      "Area under ROC: \n",
      "0.708553602426278\n",
      "-----------------------------------------------------------\n",
      "Model: weighted_xgboostclassifier\n",
      "[02:36:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Confusion matrix: \n",
      "[[2031  798]\n",
      " [  94  175]]\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.72      0.82      2829\n",
      "           1       0.18      0.65      0.28       269\n",
      "\n",
      "    accuracy                           0.71      3098\n",
      "   macro avg       0.57      0.68      0.55      3098\n",
      "weighted avg       0.89      0.71      0.77      3098\n",
      "\n",
      "Area under ROC: \n",
      "0.6842395739296007\n",
      "-----------------------------------------------------------\n",
      "Model: neural_network\n",
      "Confusion matrix: \n",
      "[[2829    0]\n",
      " [ 269    0]]\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      2829\n",
      "           1       0.00      0.00      0.00       269\n",
      "\n",
      "    accuracy                           0.91      3098\n",
      "   macro avg       0.46      0.50      0.48      3098\n",
      "weighted avg       0.83      0.91      0.87      3098\n",
      "\n",
      "Area under ROC: \n",
      "0.5\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "models = models_dict.copy()\n",
    "for key, classifier in models.items():\n",
    "    print(f\"Model: {key}\")\n",
    "    pipe = make_pipeline(processed_features, binned_features, classifier)\n",
    "    # fit the pipeline on the transformed data\n",
    "    pipe.fit(X_train, y_train)   \n",
    "    # make predictions\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    print(f\"Confusion matrix: \\n{confusion_matrix(y_test, y_pred)}\")\n",
    "    print(f\"Classification report: \\n{classification_report(y_test, y_pred)}\")\n",
    "    print(f\"Area under ROC: \\n{roc_auc_score(y_test, y_pred)}\")\n",
    "    print(\"-----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted Linear SVM model performs well on recall for the positive class as well as on the AUROC metric. However, it has the lowest accuracy.\n",
    "\n",
    "In terms of accuracy scores, the ANN performs very well at 91%, followed by XGBoost with 71%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process will be performed on a Linear SVM model as it has the highest recall and second highest AUROC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUROC: 0.7944\n",
      "Gini: 0.5889\n"
     ]
    }
   ],
   "source": [
    "model = models_dict[\"weighted_svm\"]\n",
    "\n",
    "pipe = make_pipeline(processed_features, binned_features, model)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "# Fit and evaluate the pipeline with cross-validation\n",
    "scores = cross_val_score(pipe, X_train, y_train, scoring='roc_auc', cv=cv)\n",
    "AUROC = np.mean(scores)\n",
    "GINI = AUROC * 2 - 1\n",
    "\n",
    "# Print the mean AUROC score and Gini\n",
    "print('Mean AUROC: %.4f' % (AUROC))\n",
    "print('Gini: %.4f' % (GINI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RepeatedStratifiedKFold, the AUROC improved by approximately 9%, thus this model can be used for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(df):\n",
    "    df = process_features(df)\n",
    "    df = numerical_features_binning(df)\n",
    "    print(f\"Shape of dataset: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (3098, 42)\n",
      "Confusion matrix: \n",
      "[[1811 1018]\n",
      " [  60  209]]\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.64      0.77      2829\n",
      "           1       0.17      0.78      0.28       269\n",
      "\n",
      "    accuracy                           0.65      3098\n",
      "   macro avg       0.57      0.71      0.53      3098\n",
      "weighted avg       0.90      0.65      0.73      3098\n",
      "\n",
      "Area under ROC: \n",
      "0.708553602426278\n"
     ]
    }
   ],
   "source": [
    "X_test_tr = transform_df(X_test)\n",
    "\n",
    "y_pred = model.predict(X_test_tr)\n",
    "print(f\"Confusion matrix: \\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"Classification report: \\n{classification_report(y_test, y_pred)}\")\n",
    "print(f\"Area under ROC: \\n{roc_auc_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the values required to plot a ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "# plot the ROC curve\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "# plot a secondary diagonal line, with dashed line style and black color to represent a no-skill classifier\n",
    "plt.plot(fpr, fpr, linestyle = '--', color = 'k')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model on the test set returns a 78% recall and 71% AUROC scores which are decent and similar to the evaluation from the training dataset.  \n",
    "\n",
    "The ROC Curve does look a little suspicious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I will generate a credit score for each customer, and a recommendation on the optimal credit score cutoff point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Feature Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature name</th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>0.001354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>outstanding_-1M</td>\n",
       "      <td>0.072725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>outstanding_1-2M</td>\n",
       "      <td>-0.034422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>outstanding_2M-</td>\n",
       "      <td>-0.036949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bill_-1M</td>\n",
       "      <td>0.011819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bill_1-3M</td>\n",
       "      <td>-0.051301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bill_3-5M</td>\n",
       "      <td>-0.017608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bill_5M-</td>\n",
       "      <td>0.058445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>total_retail_usage_-300K</td>\n",
       "      <td>0.101545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>total_retail_usage_300K-2.5M</td>\n",
       "      <td>-0.098794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>total_retail_usage_2.5M-</td>\n",
       "      <td>-0.001397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>overlimit_percentage_2.49</td>\n",
       "      <td>-0.110212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>overlimit_percentage_2.49M-7.47M</td>\n",
       "      <td>0.013893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>overlimit_percentage_7.47M-17.43M</td>\n",
       "      <td>0.046050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>overlimit_percentage_17.43M-</td>\n",
       "      <td>0.051623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>payment_ratio_6month_-37.477</td>\n",
       "      <td>0.078157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>payment_ratio_6month_37.477-603.985</td>\n",
       "      <td>-0.079039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>payment_ratio_6month_603.985-</td>\n",
       "      <td>0.002236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>total_usage_-305K</td>\n",
       "      <td>0.086352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>total_usage_305K-</td>\n",
       "      <td>-0.084998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>remaining_bill_per_number_of_cards_-1M</td>\n",
       "      <td>-0.051708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>remaining_bill_per_number_of_cards_1M-3M</td>\n",
       "      <td>-0.012529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>remaining_bill_per_number_of_cards_3M-</td>\n",
       "      <td>0.065591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>remaining_bill_per_limit_-1</td>\n",
       "      <td>-0.075979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>remaining_bill_per_limit_1-2</td>\n",
       "      <td>0.072948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>remaining_bill_per_limit_2-</td>\n",
       "      <td>0.004385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>total_usage_per_limit_-0.0208</td>\n",
       "      <td>0.121162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>total_usage_per_limit_0.0208-0.51</td>\n",
       "      <td>-0.099097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>total_usage_per_limit_0.51-0.674</td>\n",
       "      <td>-0.010346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>total_usage_per_limit_0.674-0.837</td>\n",
       "      <td>-0.002978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>total_usage_per_limit_0.837-</td>\n",
       "      <td>-0.007387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>total_3mo_usage_per_limit_-0.0992</td>\n",
       "      <td>0.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>total_3mo_usage_per_limit_0.0992-0.55</td>\n",
       "      <td>-0.047389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>total_3mo_usage_per_limit_0.55-</td>\n",
       "      <td>-0.003457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>total_6mo_usage_per_limit_-0.0724</td>\n",
       "      <td>0.021496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>total_6mo_usage_per_limit_0.0724-</td>\n",
       "      <td>-0.020142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>utilization_3month_-0.588</td>\n",
       "      <td>-0.047694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>utilization_3month_0.588-1.176</td>\n",
       "      <td>0.020122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>utilization_3month_1.176-2.352</td>\n",
       "      <td>0.025141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>utilization_3month_2.352-</td>\n",
       "      <td>0.003784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>utilization_6month_-0.734</td>\n",
       "      <td>-0.011558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>utilization_6month_0.734-2.202</td>\n",
       "      <td>0.011456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>utilization_6month_2.202-</td>\n",
       "      <td>0.001455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Feature name  Coefficients\n",
       "0                                  Intercept      0.001354\n",
       "1                            outstanding_-1M      0.072725\n",
       "2                           outstanding_1-2M     -0.034422\n",
       "3                            outstanding_2M-     -0.036949\n",
       "4                                   bill_-1M      0.011819\n",
       "5                                  bill_1-3M     -0.051301\n",
       "6                                  bill_3-5M     -0.017608\n",
       "7                                   bill_5M-      0.058445\n",
       "8                   total_retail_usage_-300K      0.101545\n",
       "9               total_retail_usage_300K-2.5M     -0.098794\n",
       "10                  total_retail_usage_2.5M-     -0.001397\n",
       "11                 overlimit_percentage_2.49     -0.110212\n",
       "12          overlimit_percentage_2.49M-7.47M      0.013893\n",
       "13         overlimit_percentage_7.47M-17.43M      0.046050\n",
       "14              overlimit_percentage_17.43M-      0.051623\n",
       "15              payment_ratio_6month_-37.477      0.078157\n",
       "16       payment_ratio_6month_37.477-603.985     -0.079039\n",
       "17             payment_ratio_6month_603.985-      0.002236\n",
       "18                         total_usage_-305K      0.086352\n",
       "19                         total_usage_305K-     -0.084998\n",
       "20    remaining_bill_per_number_of_cards_-1M     -0.051708\n",
       "21  remaining_bill_per_number_of_cards_1M-3M     -0.012529\n",
       "22    remaining_bill_per_number_of_cards_3M-      0.065591\n",
       "23               remaining_bill_per_limit_-1     -0.075979\n",
       "24              remaining_bill_per_limit_1-2      0.072948\n",
       "25               remaining_bill_per_limit_2-      0.004385\n",
       "26             total_usage_per_limit_-0.0208      0.121162\n",
       "27         total_usage_per_limit_0.0208-0.51     -0.099097\n",
       "28          total_usage_per_limit_0.51-0.674     -0.010346\n",
       "29         total_usage_per_limit_0.674-0.837     -0.002978\n",
       "30              total_usage_per_limit_0.837-     -0.007387\n",
       "31         total_3mo_usage_per_limit_-0.0992      0.052200\n",
       "32     total_3mo_usage_per_limit_0.0992-0.55     -0.047389\n",
       "33           total_3mo_usage_per_limit_0.55-     -0.003457\n",
       "34         total_6mo_usage_per_limit_-0.0724      0.021496\n",
       "35         total_6mo_usage_per_limit_0.0724-     -0.020142\n",
       "36                 utilization_3month_-0.588     -0.047694\n",
       "37            utilization_3month_0.588-1.176      0.020122\n",
       "38            utilization_3month_1.176-2.352      0.025141\n",
       "39                 utilization_3month_2.352-      0.003784\n",
       "40                 utilization_6month_-0.734     -0.011558\n",
       "41            utilization_6month_0.734-2.202      0.011456\n",
       "42                 utilization_6month_2.202-      0.001455"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the column names in X_train as a list\n",
    "feature_name = ['outstanding_-1M', 'outstanding_1-2M', 'outstanding_2M-', 'bill_-1M',\n",
    "       'bill_1-3M', 'bill_3-5M', 'bill_5M-', 'total_retail_usage_-300K',\n",
    "       'total_retail_usage_300K-2.5M', 'total_retail_usage_2.5M-',\n",
    "       'overlimit_percentage_2.49', 'overlimit_percentage_2.49M-7.47M',\n",
    "       'overlimit_percentage_7.47M-17.43M', 'overlimit_percentage_17.43M-',\n",
    "       'payment_ratio_6month_-37.477', 'payment_ratio_6month_37.477-603.985',\n",
    "       'payment_ratio_6month_603.985-', 'total_usage_-305K',\n",
    "       'total_usage_305K-', 'remaining_bill_per_number_of_cards_-1M',\n",
    "       'remaining_bill_per_number_of_cards_1M-3M',\n",
    "       'remaining_bill_per_number_of_cards_3M-', 'remaining_bill_per_limit_-1',\n",
    "       'remaining_bill_per_limit_1-2', 'remaining_bill_per_limit_2-',\n",
    "       'total_usage_per_limit_-0.0208', 'total_usage_per_limit_0.0208-0.51',\n",
    "       'total_usage_per_limit_0.51-0.674', 'total_usage_per_limit_0.674-0.837',\n",
    "       'total_usage_per_limit_0.837-', 'total_3mo_usage_per_limit_-0.0992',\n",
    "       'total_3mo_usage_per_limit_0.0992-0.55',\n",
    "       'total_3mo_usage_per_limit_0.55-', 'total_6mo_usage_per_limit_-0.0724',\n",
    "       'total_6mo_usage_per_limit_0.0724-', 'utilization_3month_-0.588',\n",
    "       'utilization_3month_0.588-1.176', 'utilization_3month_1.176-2.352',\n",
    "       'utilization_3month_2.352-', 'utilization_6month_-0.734',\n",
    "       'utilization_6month_0.734-2.202', 'utilization_6month_2.202-']\n",
    "\n",
    "# Create a summary table of the model\n",
    "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
    "\n",
    "# Create a new column in the dataframe, called 'Coefficients', with row values the transposed coefficients from the model\n",
    "summary_table['Coefficients'] = np.transpose(model.coef_)\n",
    "\n",
    "# Increase the index of every row of the dataframe with 1 to store our model intercept in 1st row\n",
    "summary_table.index = summary_table.index + 1\n",
    "\n",
    "# Assign our model intercept to this new row\n",
    "summary_table.loc[0] = ['Intercept', model.intercept_[0]]\n",
    "\n",
    "# Sort the dataframe by index\n",
    "summary_table.sort_index(inplace = True)\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model score to points system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score = (β×WoE+ α/n)×Factor + Offset/n\n",
    "\n",
    "Where:  \n",
    "β = model coefficient for characteristics that contains the given attribute  \n",
    "α = model intercept  \n",
    "WoE = Weight of Evidence value for the given attribute  \n",
    "n = number of characteristics included in the model  \n",
    "Factor = pdo/Ln(2)  \n",
    "Offset = Score — (Factor × ln(Odds))\n",
    "\n",
    "For this exercise, I will fit the coefficient within a score threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the min and max threshholds for scorecard\n",
    "min_score = 300\n",
    "max_score = 850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature name</th>\n",
       "      <th>Coefficients</th>\n",
       "      <th>Original feature name</th>\n",
       "      <th>Score - Calculation</th>\n",
       "      <th>Score - Preliminary</th>\n",
       "      <th>Difference</th>\n",
       "      <th>Score - Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>Intercept</td>\n",
       "      <td>580.629415</td>\n",
       "      <td>581.0</td>\n",
       "      <td>0.370585</td>\n",
       "      <td>581.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>outstanding_-1M</td>\n",
       "      <td>0.072725</td>\n",
       "      <td>outstanding</td>\n",
       "      <td>40.795242</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.204758</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>outstanding_1-2M</td>\n",
       "      <td>-0.034422</td>\n",
       "      <td>outstanding</td>\n",
       "      <td>-19.308972</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>0.308972</td>\n",
       "      <td>-19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>outstanding_2M-</td>\n",
       "      <td>-0.036949</td>\n",
       "      <td>outstanding</td>\n",
       "      <td>-20.726795</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>-0.273205</td>\n",
       "      <td>-21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bill_-1M</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>bill</td>\n",
       "      <td>6.629806</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.370194</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bill_1-3M</td>\n",
       "      <td>-0.051301</td>\n",
       "      <td>bill</td>\n",
       "      <td>-28.777740</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>-0.222260</td>\n",
       "      <td>-29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bill_3-5M</td>\n",
       "      <td>-0.017608</td>\n",
       "      <td>bill</td>\n",
       "      <td>-9.877459</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.122541</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bill_5M-</td>\n",
       "      <td>0.058445</td>\n",
       "      <td>bill</td>\n",
       "      <td>32.784867</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.215133</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>total_retail_usage_-300K</td>\n",
       "      <td>0.101545</td>\n",
       "      <td>total</td>\n",
       "      <td>56.961938</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>total_retail_usage_300K-2.5M</td>\n",
       "      <td>-0.098794</td>\n",
       "      <td>total</td>\n",
       "      <td>-55.418792</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>0.418792</td>\n",
       "      <td>-55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>total_retail_usage_2.5M-</td>\n",
       "      <td>-0.001397</td>\n",
       "      <td>total</td>\n",
       "      <td>-0.783671</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.216329</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>overlimit_percentage_2.49</td>\n",
       "      <td>-0.110212</td>\n",
       "      <td>overlimit</td>\n",
       "      <td>-61.823894</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-0.176106</td>\n",
       "      <td>-62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>overlimit_percentage_2.49M-7.47M</td>\n",
       "      <td>0.013893</td>\n",
       "      <td>overlimit</td>\n",
       "      <td>7.793489</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.206511</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>overlimit_percentage_7.47M-17.43M</td>\n",
       "      <td>0.046050</td>\n",
       "      <td>overlimit</td>\n",
       "      <td>25.831936</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.168064</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>overlimit_percentage_17.43M-</td>\n",
       "      <td>0.051623</td>\n",
       "      <td>overlimit</td>\n",
       "      <td>28.957943</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.042057</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>payment_ratio_6month_-37.477</td>\n",
       "      <td>0.078157</td>\n",
       "      <td>payment</td>\n",
       "      <td>43.842673</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.157327</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>payment_ratio_6month_37.477-603.985</td>\n",
       "      <td>-0.079039</td>\n",
       "      <td>payment</td>\n",
       "      <td>-44.337402</td>\n",
       "      <td>-44.0</td>\n",
       "      <td>0.337402</td>\n",
       "      <td>-44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>payment_ratio_6month_603.985-</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>payment</td>\n",
       "      <td>1.254204</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.254204</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>total_usage_-305K</td>\n",
       "      <td>0.086352</td>\n",
       "      <td>total</td>\n",
       "      <td>48.439620</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-0.439620</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>total_usage_305K-</td>\n",
       "      <td>-0.084998</td>\n",
       "      <td>total</td>\n",
       "      <td>-47.680146</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>-0.319854</td>\n",
       "      <td>-48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>remaining_bill_per_number_of_cards_-1M</td>\n",
       "      <td>-0.051708</td>\n",
       "      <td>remaining</td>\n",
       "      <td>-29.005683</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>0.005683</td>\n",
       "      <td>-29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>remaining_bill_per_number_of_cards_1M-3M</td>\n",
       "      <td>-0.012529</td>\n",
       "      <td>remaining</td>\n",
       "      <td>-7.028336</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>0.028336</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>remaining_bill_per_number_of_cards_3M-</td>\n",
       "      <td>0.065591</td>\n",
       "      <td>remaining</td>\n",
       "      <td>36.793492</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.206508</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>remaining_bill_per_limit_-1</td>\n",
       "      <td>-0.075979</td>\n",
       "      <td>remaining</td>\n",
       "      <td>-42.620632</td>\n",
       "      <td>-43.0</td>\n",
       "      <td>-0.379368</td>\n",
       "      <td>-43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>remaining_bill_per_limit_1-2</td>\n",
       "      <td>0.072948</td>\n",
       "      <td>remaining</td>\n",
       "      <td>40.920365</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.079635</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>remaining_bill_per_limit_2-</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>remaining</td>\n",
       "      <td>2.459741</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.459741</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>total_usage_per_limit_-0.0208</td>\n",
       "      <td>0.121162</td>\n",
       "      <td>total</td>\n",
       "      <td>67.966551</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.033449</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>total_usage_per_limit_0.0208-0.51</td>\n",
       "      <td>-0.099097</td>\n",
       "      <td>total</td>\n",
       "      <td>-55.589031</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>-0.410969</td>\n",
       "      <td>-56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>total_usage_per_limit_0.51-0.674</td>\n",
       "      <td>-0.010346</td>\n",
       "      <td>total</td>\n",
       "      <td>-5.803671</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-0.196329</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>total_usage_per_limit_0.674-0.837</td>\n",
       "      <td>-0.002978</td>\n",
       "      <td>total</td>\n",
       "      <td>-1.670415</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.329585</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>total_usage_per_limit_0.837-</td>\n",
       "      <td>-0.007387</td>\n",
       "      <td>total</td>\n",
       "      <td>-4.143960</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.143960</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>total_3mo_usage_per_limit_-0.0992</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>total</td>\n",
       "      <td>29.281910</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-0.281910</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>total_3mo_usage_per_limit_0.0992-0.55</td>\n",
       "      <td>-0.047389</td>\n",
       "      <td>total</td>\n",
       "      <td>-26.583173</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>-0.416827</td>\n",
       "      <td>-27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>total_3mo_usage_per_limit_0.55-</td>\n",
       "      <td>-0.003457</td>\n",
       "      <td>total</td>\n",
       "      <td>-1.939263</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.060737</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>total_6mo_usage_per_limit_-0.0724</td>\n",
       "      <td>0.021496</td>\n",
       "      <td>total</td>\n",
       "      <td>12.058260</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.058260</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>total_6mo_usage_per_limit_0.0724-</td>\n",
       "      <td>-0.020142</td>\n",
       "      <td>total</td>\n",
       "      <td>-11.298786</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>0.298786</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>utilization_3month_-0.588</td>\n",
       "      <td>-0.047694</td>\n",
       "      <td>utilization</td>\n",
       "      <td>-26.753921</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>-0.246079</td>\n",
       "      <td>-27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>utilization_3month_0.588-1.176</td>\n",
       "      <td>0.020122</td>\n",
       "      <td>utilization</td>\n",
       "      <td>11.287645</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-0.287645</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>utilization_3month_1.176-2.352</td>\n",
       "      <td>0.025141</td>\n",
       "      <td>utilization</td>\n",
       "      <td>14.102946</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-0.102946</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>utilization_3month_2.352-</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>utilization</td>\n",
       "      <td>2.122805</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.122805</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>utilization_6month_-0.734</td>\n",
       "      <td>-0.011558</td>\n",
       "      <td>utilization</td>\n",
       "      <td>-6.483296</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>0.483296</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>utilization_6month_0.734-2.202</td>\n",
       "      <td>0.011456</td>\n",
       "      <td>utilization</td>\n",
       "      <td>6.426336</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.426336</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>utilization_6month_2.202-</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>utilization</td>\n",
       "      <td>0.816434</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.183566</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Feature name  Coefficients  \\\n",
       "0                                  Intercept      0.001354   \n",
       "1                            outstanding_-1M      0.072725   \n",
       "2                           outstanding_1-2M     -0.034422   \n",
       "3                            outstanding_2M-     -0.036949   \n",
       "4                                   bill_-1M      0.011819   \n",
       "5                                  bill_1-3M     -0.051301   \n",
       "6                                  bill_3-5M     -0.017608   \n",
       "7                                   bill_5M-      0.058445   \n",
       "8                   total_retail_usage_-300K      0.101545   \n",
       "9               total_retail_usage_300K-2.5M     -0.098794   \n",
       "10                  total_retail_usage_2.5M-     -0.001397   \n",
       "11                 overlimit_percentage_2.49     -0.110212   \n",
       "12          overlimit_percentage_2.49M-7.47M      0.013893   \n",
       "13         overlimit_percentage_7.47M-17.43M      0.046050   \n",
       "14              overlimit_percentage_17.43M-      0.051623   \n",
       "15              payment_ratio_6month_-37.477      0.078157   \n",
       "16       payment_ratio_6month_37.477-603.985     -0.079039   \n",
       "17             payment_ratio_6month_603.985-      0.002236   \n",
       "18                         total_usage_-305K      0.086352   \n",
       "19                         total_usage_305K-     -0.084998   \n",
       "20    remaining_bill_per_number_of_cards_-1M     -0.051708   \n",
       "21  remaining_bill_per_number_of_cards_1M-3M     -0.012529   \n",
       "22    remaining_bill_per_number_of_cards_3M-      0.065591   \n",
       "23               remaining_bill_per_limit_-1     -0.075979   \n",
       "24              remaining_bill_per_limit_1-2      0.072948   \n",
       "25               remaining_bill_per_limit_2-      0.004385   \n",
       "26             total_usage_per_limit_-0.0208      0.121162   \n",
       "27         total_usage_per_limit_0.0208-0.51     -0.099097   \n",
       "28          total_usage_per_limit_0.51-0.674     -0.010346   \n",
       "29         total_usage_per_limit_0.674-0.837     -0.002978   \n",
       "30              total_usage_per_limit_0.837-     -0.007387   \n",
       "31         total_3mo_usage_per_limit_-0.0992      0.052200   \n",
       "32     total_3mo_usage_per_limit_0.0992-0.55     -0.047389   \n",
       "33           total_3mo_usage_per_limit_0.55-     -0.003457   \n",
       "34         total_6mo_usage_per_limit_-0.0724      0.021496   \n",
       "35         total_6mo_usage_per_limit_0.0724-     -0.020142   \n",
       "36                 utilization_3month_-0.588     -0.047694   \n",
       "37            utilization_3month_0.588-1.176      0.020122   \n",
       "38            utilization_3month_1.176-2.352      0.025141   \n",
       "39                 utilization_3month_2.352-      0.003784   \n",
       "40                 utilization_6month_-0.734     -0.011558   \n",
       "41            utilization_6month_0.734-2.202      0.011456   \n",
       "42                 utilization_6month_2.202-      0.001455   \n",
       "\n",
       "   Original feature name  Score - Calculation  Score - Preliminary  \\\n",
       "0              Intercept           580.629415                581.0   \n",
       "1            outstanding            40.795242                 41.0   \n",
       "2            outstanding           -19.308972                -19.0   \n",
       "3            outstanding           -20.726795                -21.0   \n",
       "4                   bill             6.629806                  7.0   \n",
       "5                   bill           -28.777740                -29.0   \n",
       "6                   bill            -9.877459                -10.0   \n",
       "7                   bill            32.784867                 33.0   \n",
       "8                  total            56.961938                 57.0   \n",
       "9                  total           -55.418792                -55.0   \n",
       "10                 total            -0.783671                 -1.0   \n",
       "11             overlimit           -61.823894                -62.0   \n",
       "12             overlimit             7.793489                  8.0   \n",
       "13             overlimit            25.831936                 26.0   \n",
       "14             overlimit            28.957943                 29.0   \n",
       "15               payment            43.842673                 44.0   \n",
       "16               payment           -44.337402                -44.0   \n",
       "17               payment             1.254204                  1.0   \n",
       "18                 total            48.439620                 48.0   \n",
       "19                 total           -47.680146                -48.0   \n",
       "20             remaining           -29.005683                -29.0   \n",
       "21             remaining            -7.028336                 -7.0   \n",
       "22             remaining            36.793492                 37.0   \n",
       "23             remaining           -42.620632                -43.0   \n",
       "24             remaining            40.920365                 41.0   \n",
       "25             remaining             2.459741                  2.0   \n",
       "26                 total            67.966551                 68.0   \n",
       "27                 total           -55.589031                -56.0   \n",
       "28                 total            -5.803671                 -6.0   \n",
       "29                 total            -1.670415                 -2.0   \n",
       "30                 total            -4.143960                 -4.0   \n",
       "31                 total            29.281910                 29.0   \n",
       "32                 total           -26.583173                -27.0   \n",
       "33                 total            -1.939263                 -2.0   \n",
       "34                 total            12.058260                 12.0   \n",
       "35                 total           -11.298786                -11.0   \n",
       "36           utilization           -26.753921                -27.0   \n",
       "37           utilization            11.287645                 11.0   \n",
       "38           utilization            14.102946                 14.0   \n",
       "39           utilization             2.122805                  2.0   \n",
       "40           utilization            -6.483296                 -6.0   \n",
       "41           utilization             6.426336                  6.0   \n",
       "42           utilization             0.816434                  1.0   \n",
       "\n",
       "    Difference  Score - Final  \n",
       "0     0.370585          581.0  \n",
       "1     0.204758           41.0  \n",
       "2     0.308972          -19.0  \n",
       "3    -0.273205          -21.0  \n",
       "4     0.370194            7.0  \n",
       "5    -0.222260          -29.0  \n",
       "6    -0.122541          -10.0  \n",
       "7     0.215133           33.0  \n",
       "8     0.038062           57.0  \n",
       "9     0.418792          -55.0  \n",
       "10   -0.216329           -1.0  \n",
       "11   -0.176106          -62.0  \n",
       "12    0.206511            8.0  \n",
       "13    0.168064           26.0  \n",
       "14    0.042057           29.0  \n",
       "15    0.157327           44.0  \n",
       "16    0.337402          -44.0  \n",
       "17   -0.254204            1.0  \n",
       "18   -0.439620           48.0  \n",
       "19   -0.319854          -48.0  \n",
       "20    0.005683          -29.0  \n",
       "21    0.028336           -7.0  \n",
       "22    0.206508           37.0  \n",
       "23   -0.379368          -43.0  \n",
       "24    0.079635           41.0  \n",
       "25   -0.459741            2.0  \n",
       "26    0.033449           68.0  \n",
       "27   -0.410969          -56.0  \n",
       "28   -0.196329           -6.0  \n",
       "29   -0.329585           -2.0  \n",
       "30    0.143960           -4.0  \n",
       "31   -0.281910           29.0  \n",
       "32   -0.416827          -27.0  \n",
       "33   -0.060737           -2.0  \n",
       "34   -0.058260           12.0  \n",
       "35    0.298786          -11.0  \n",
       "36   -0.246079          -27.0  \n",
       "37   -0.287645           11.0  \n",
       "38   -0.102946           14.0  \n",
       "39   -0.122805            2.0  \n",
       "40    0.483296           -6.0  \n",
       "41   -0.426336            6.0  \n",
       "42    0.183566            1.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scorecard = summary_table.copy()\n",
    "df_scorecard['Original feature name'] = summary_table['Feature name'].str.split('_').str[0]\n",
    "\n",
    "# calculate the sum of the minimum coefficients of each category within the original feature name\n",
    "min_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].min().sum()\n",
    "\n",
    "# calculate the sum of the maximum coefficients of each category within the original feature name\n",
    "max_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].max().sum()\n",
    "\n",
    "# create a new columns that has the imputed calculated Score based on the multiplication of the coefficient by the ratio of the differences between\n",
    "# maximum & minimum score and maximum & minimum sum of cefficients.\n",
    "df_scorecard['Score - Calculation'] = df_scorecard['Coefficients'] * (max_score - min_score) / (max_sum_coef - min_sum_coef)\n",
    "\n",
    "# update the calculated score of the Intercept (i.e. the default score for each loan)\n",
    "df_scorecard.loc[0, 'Score - Calculation'] = ((df_scorecard.loc[0,'Coefficients'] - min_sum_coef) / (max_sum_coef - min_sum_coef)) * (max_score - min_score) + min_score\n",
    "\n",
    "# round the values of the 'Score - Calculation' column and store them in a new column\n",
    "df_scorecard['Score - Preliminary'] = df_scorecard['Score - Calculation'].round()\n",
    "\n",
    "df_scorecard['Difference'] = df_scorecard['Score - Preliminary'] - df_scorecard['Score - Calculation']\n",
    "df_scorecard['Score - Final'] = df_scorecard['Score - Preliminary']\n",
    "\n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column \"Score - Final\" contains the scoring weights of each transformed feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific statistical method for credit scoring or overall risk assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Credit Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3098, 43)\n",
      "(43, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>236.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15204</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12226</th>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7566</th>\n",
       "      <td>706.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3575</th>\n",
       "      <td>617.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "1803   236.0\n",
       "15204  500.0\n",
       "12226  159.0\n",
       "7566   706.0\n",
       "3575   617.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_test_tr_int = pd.DataFrame(X_test_tr.insert(0, 'Intercept', 1))\n",
    "X_test_tr_int = pd.DataFrame(X_test_tr)\n",
    "\n",
    "# get the list of our final scorecard scores\n",
    "scorecard_scores = df_scorecard['Score - Final']\n",
    "scorecard_scores = scorecard_scores.values.reshape(43, 1)\n",
    "\n",
    "print(X_test_tr_int.shape)\n",
    "print(scorecard_scores.shape)\n",
    "\n",
    "# matrix dot multiplication of test set with scorecard scores\n",
    "y_scores = X_test_tr_int.dot(scorecard_scores)\n",
    "y_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is an individual user, and their respective scores are in the column \"0\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Credit Score Cut-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate overall default rate of portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate Youden's J-Statistic to identify the best threshhold\n",
    "J = tpr - fpr\n",
    "# locate the index of the largest J\n",
    "ix = np.argmax(J)\n",
    "best_thresh = thresholds[ix]\n",
    "print('Best Threshold: %f' % (best_thresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that based on the Youden's J statistic, the ideal probability threshold is 1 which minimizes the FPR and maximimizes the TPR - all samples with a predicted probability higher than this should be classified as in Default and vice versa. However, this is not realistically feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk and default levels by deciles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create deciles by credit score and provide risk and default levels by deciles (by decile and cumulative)\n",
    "- Confidence for scores/default rates by bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use SMOTE to balance dataset\n",
    "- Perform further feature engineering for numerical features:\n",
    "    - ratios of features\n",
    "    - log scaling for features with high values \n",
    "    - normalization\n",
    "- Determine feature importances\n",
    "- Use different models, i.e:\n",
    "    - Logistic Regression\n",
    "    - Light Gradient Boosting Machine\n",
    "- Perform hyper-parameter tuning\n",
    "- Use an ensemble of models\n",
    "- Create function to generate scorecard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
